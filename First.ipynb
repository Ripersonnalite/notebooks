{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ceeba1-cd79-4c7f-80ee-f5cb29d838f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mWarning: statements after `dbutils.library.restartPython()` will execute before Python is restarted.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Key Library Explanations:\n",
    "\n",
    "Data Processing and Analysis:\n",
    "- pandas (as pd): Library for data manipulation and analysis, provides DataFrame structures\n",
    "- numpy: Often used with pandas for numerical operations (though not directly imported in your code)\n",
    "\n",
    "Financial Data:\n",
    "- yfinance (as yf): Downloads financial market data from Yahoo Finance\n",
    "                    Provides historical prices, company info, and financial statements\n",
    "                    \n",
    "Technical Analysis:\n",
    "- ta: Technical analysis library with 130+ indicators including:\n",
    "      - Momentum indicators (RSI, MACD)\n",
    "      - Volume indicators\n",
    "      - Volatility indicators (Bollinger Bands)\n",
    "      - Trend indicators (Moving Averages)\n",
    "\n",
    "Visualization:\n",
    "- matplotlib.pyplot (as plt): Creating static, animated, and interactive visualizations\n",
    "\n",
    "Spark and Big Data:\n",
    "- pyspark.sql: Apache Spark's module for structured data processing\n",
    "- SparkSession: Entry point for DataFrame and SQL functionality\n",
    "- functions (as F): Spark SQL functions for data transformations\n",
    "- Window (as W): For window-based operations like running totals\n",
    "- StructType/Field: Define schema for Spark DataFrames\n",
    "\n",
    "System and Utilities:\n",
    "- subprocess: Running external commands and managing subprocesses\n",
    "- sys: Python runtime environment access\n",
    "- gc: Garbage collection for memory management\n",
    "- warnings: Control warning messages\n",
    "- time.sleep: Add delays in code execution\n",
    "\n",
    "Email Functionality:\n",
    "- smtplib: Send emails using SMTP protocol\n",
    "- email.mime.text: Create and format email messages\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def install_quietly(packages):\n",
    "    \"\"\"Install packages quietly using pip.\"\"\"\n",
    "    for package in packages:\n",
    "        try:\n",
    "            with open(os.devnull, 'w') as devnull:\n",
    "                subprocess.check_call(\n",
    "                    [sys.executable, '-m', 'pip', 'install', '--quiet', package],\n",
    "                    stdout=devnull,\n",
    "                    stderr=subprocess.STDOUT\n",
    "                )\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error installing {package}: {str(e)}\")\n",
    "\n",
    "# Required packages\n",
    "packages = [\n",
    "    'pandas',\n",
    "    'yfinance', \n",
    "    'ta',\n",
    "    'matplotlib'\n",
    "]\n",
    "\n",
    "try:\n",
    "    install_quietly(packages)\n",
    "    dbutils.library.restartPython()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89a5705-b65a-42be-bb70-1116e4931c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped existing table if it existed\n\nTotal stocks to process: 4\n\nProcessing AAPL\nSuccessfully processed AAPL - Apple Inc.\n\nProcessing MSFT\nSuccessfully processed MSFT - Microsoft Corporation\n\nProcessing TSLA\nSuccessfully processed TSLA - Tesla, Inc.\n\nProcessing NVDA\nSuccessfully processed NVDA - NVIDIA Corporation\n\nSuccessfully wrote data to bronze table\n\nVerifying final data - showing stocks and their record counts:\n+------+---------------------+------------+-------------+-------------+-------------------+-------------------+\n|Ticker|Company_Name         |record_count|rows_with_sma|rows_with_rsi|earliest_date      |latest_date        |\n+------+---------------------+------------+-------------+-------------+-------------------+-------------------+\n|AAPL  |Apple Inc.           |11097       |11097        |11097        |1980-12-12 05:00:00|2024-12-18 05:00:00|\n|MSFT  |Microsoft Corporation|9771        |9771         |9771         |1986-03-13 05:00:00|2024-12-18 05:00:00|\n|NVDA  |NVIDIA Corporation   |6520        |6520         |6520         |1999-01-22 05:00:00|2024-12-18 05:00:00|\n|TSLA  |Tesla, Inc.          |3644        |3644         |3644         |2010-06-29 04:00:00|2024-12-18 05:00:00|\n+------+---------------------+------------+-------------+-------------+-------------------+-------------------+\n\n\nScript completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data Processing and Analysis\n",
    "import pandas as pd                     # Data manipulation and analysis library\n",
    "from pyspark.sql import SparkSession    # Entry point for Spark SQL and DataFrame\n",
    "from pyspark.sql.types import *         # Spark SQL data types for schema definition\n",
    "from pyspark.sql import functions as F   # Spark SQL functions for data transformations\n",
    "from pyspark.sql import Window as W      # Window functions for operations like rolling calculations\n",
    "\n",
    "# Financial Data and Technical Analysis\n",
    "import yfinance as yf                   # Yahoo Finance API wrapper for market data\n",
    "import ta                               # Technical analysis library for financial indicators\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt         # Data visualization library\n",
    "\n",
    "# System Utilities\n",
    "import gc                              # Garbage collector for memory management\n",
    "import sys                             # System-specific parameters and functions\n",
    "from time import sleep                 # Time-related functions, used for delays\n",
    "\n",
    "# Email Communications\n",
    "import smtplib                         # SMTP protocol client for sending emails\n",
    "from email.mime.text import MIMEText   # MIME text formatting for email content\n",
    "\n",
    "def send_message(subject=\"Email by Python\", body=\"Default by Python\"):\n",
    "    \"\"\"By Ricardo Kazuo\"\"\"\n",
    "    # Read credentials and addresses from files\n",
    "    with open('/Volumes/workspace/default/data/gmail.txt', 'r') as file:\n",
    "        password = file.read().strip()\n",
    "    \n",
    "    with open('/Volumes/workspace/default/data/sender.txt', 'r') as file:\n",
    "        sender_email = file.read().strip()\n",
    "        \n",
    "    with open('/Volumes/workspace/default/data/receiver.txt', 'r') as file:\n",
    "        receiver_email = file.read().strip()\n",
    "        \n",
    "    message = MIMEText(body)\n",
    "    message['to'] = receiver_email\n",
    "    message['from'] = f\"Databricks - Extractor<{sender_email}>\"\n",
    "    message['subject'] = subject\n",
    "    server = smtplib.SMTP('smtp.gmail.com:587')\n",
    "    server.ehlo_or_helo_if_needed()\n",
    "    server.starttls()\n",
    "    server.ehlo_or_helo_if_needed()\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(f\"Databricks - Extractor<{sender_email}>\", receiver_email, message.as_string())\n",
    "    server.quit()\n",
    "\n",
    "def process_financial_data(df):\n",
    "    \"\"\"\n",
    "    Process financial data and add technical indicators\n",
    "    \"\"\"\n",
    "    # Convert required columns to numeric\n",
    "    df['Close_Price'] = pd.to_numeric(df['Close'])\n",
    "    df['Volume_Num'] = pd.to_numeric(df['Volume'])\n",
    "    \n",
    "    # Calculate Technical Indicators\n",
    "    # Moving Averages\n",
    "    df['SMA_20'] = df['Close_Price'].rolling(window=20).mean()\n",
    "    df['EMA_20'] = df['Close_Price'].ewm(span=20, adjust=False).mean()\n",
    "    \n",
    "    # RSI\n",
    "    df['RSI'] = ta.momentum.RSIIndicator(df['Close_Price']).rsi()\n",
    "    \n",
    "    # MACD\n",
    "    macd = ta.trend.MACD(df['Close_Price'])\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_Signal'] = macd.macd_signal()\n",
    "    df['MACD_Histogram'] = macd.macd_diff()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bollinger = ta.volatility.BollingerBands(df['Close_Price'])\n",
    "    df['BB_Upper'] = bollinger.bollinger_hband()\n",
    "    df['BB_Lower'] = bollinger.bollinger_lband()\n",
    "    df['BB_Middle'] = bollinger.bollinger_mavg()\n",
    "    \n",
    "    # Volume Indicators\n",
    "    df['Volume_SMA_20'] = df['Volume_Num'].rolling(window=20).mean()\n",
    "    \n",
    "    # Price Momentum\n",
    "    df['Price_Change'] = df['Close_Price'].pct_change()\n",
    "    df['Price_Change_20D'] = df['Close_Price'].pct_change(periods=20)\n",
    "    \n",
    "    # Add time-based features\n",
    "    df['Date_Time'] = pd.to_datetime(df['Date'])\n",
    "    df['day_of_week'] = df['Date_Time'].dt.dayofweek\n",
    "    df['month'] = df['Date_Time'].dt.month\n",
    "    df['year'] = df['Date_Time'].dt.year\n",
    "    df['is_month_end'] = df['Date_Time'].dt.is_month_end\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create or get Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS default.bronze_financial_stocks\")\n",
    "print(\"Dropped existing table if it existed\")\n",
    "\n",
    "# Define the specific tickers we want to process\n",
    "ticker_info = [\n",
    "    {\"ticker\": \"AAPL\", \"company_name\": \"Apple Inc.\"},\n",
    "    {\"ticker\": \"MSFT\", \"company_name\": \"Microsoft Corporation\"},\n",
    "    {\"ticker\": \"TSLA\", \"company_name\": \"Tesla, Inc.\"},\n",
    "    {\"ticker\": \"NVDA\", \"company_name\": \"NVIDIA Corporation\"}\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal stocks to process: {len(ticker_info)}\")\n",
    "\n",
    "# Send start notification\n",
    "send_message(\n",
    "    subject=\"Started Processing Selected Stocks\",\n",
    "    body=\"Beginning to fetch and process data for AAPL, MSFT, TSLA, and NVDA.\"\n",
    ")\n",
    "\n",
    "# Initialize an empty list for the data\n",
    "financial_data = []\n",
    "\n",
    "# Process each ticker\n",
    "for info in ticker_info:\n",
    "    ticker_symbol = info['ticker']\n",
    "    try:\n",
    "        print(f\"\\nProcessing {ticker_symbol}\")\n",
    "        # Get historical data\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        hist_data = ticker.history(period=\"max\")\n",
    "        \n",
    "        if hist_data.empty:\n",
    "            print(f\"No data found for {ticker_symbol}\")\n",
    "            continue\n",
    "            \n",
    "        # Reset index and add ticker column\n",
    "        hist_data = hist_data.reset_index()\n",
    "        hist_data['Ticker'] = ticker_symbol\n",
    "        hist_data['Company_Name'] = info['company_name']\n",
    "        hist_data['Index'] = 'N/A'\n",
    "        \n",
    "        # Process the data with technical indicators\n",
    "        processed_data = process_financial_data(hist_data)\n",
    "        \n",
    "        # Append to data list\n",
    "        financial_data.append(processed_data)\n",
    "        print(f\"Successfully processed {ticker_symbol} - {info['company_name']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker_symbol}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Process the collected data\n",
    "if financial_data:\n",
    "    try:\n",
    "        combined_data = pd.concat(financial_data, ignore_index=True)\n",
    "        \n",
    "        # Convert to list of rows\n",
    "        data_rows = combined_data.to_dict('records')\n",
    "        \n",
    "        # Define schema including technical indicators\n",
    "        # Define schema including technical indicators\n",
    "        schema = StructType([\n",
    "            # Basic Stock Data (original fields from yfinance)\n",
    "            StructField(\"Date\", DateType(), True),           \n",
    "            StructField(\"Open\", DoubleType(), True),         \n",
    "            StructField(\"High\", DoubleType(), True),          \n",
    "            StructField(\"Low\", DoubleType(), True),           \n",
    "            StructField(\"Close\", DoubleType(), True),        \n",
    "            StructField(\"Volume\", DoubleType(), True),       \n",
    "            StructField(\"Dividends\", DoubleType(), True),    \n",
    "            StructField(\"Stock_Splits\", DoubleType(), True), \n",
    "            \n",
    "            # Stock Identification (these were missing)\n",
    "            StructField(\"Ticker\", StringType(), True),       # Stock symbol\n",
    "            StructField(\"Company_Name\", StringType(), True), # Company name\n",
    "            StructField(\"Index\", StringType(), True),        # Index identifier\n",
    "            \n",
    "            # Your calculated fields (remain the same)\n",
    "            StructField(\"Close_Price\", DoubleType(), True),  \n",
    "            StructField(\"Volume_Num\", DoubleType(), True),   \n",
    "            StructField(\"SMA_20\", DoubleType(), True),      \n",
    "            StructField(\"EMA_20\", DoubleType(), True),      \n",
    "            StructField(\"RSI\", DoubleType(), True),         \n",
    "            StructField(\"MACD\", DoubleType(), True),        \n",
    "            StructField(\"MACD_Signal\", DoubleType(), True), \n",
    "            StructField(\"MACD_Histogram\", DoubleType(), True), \n",
    "            StructField(\"BB_Upper\", DoubleType(), True),    \n",
    "            StructField(\"BB_Lower\", DoubleType(), True),    \n",
    "            StructField(\"BB_Middle\", DoubleType(), True),   \n",
    "            StructField(\"Volume_SMA_20\", DoubleType(), True), \n",
    "            StructField(\"Price_Change\", DoubleType(), True),    \n",
    "            StructField(\"Price_Change_20D\", DoubleType(), True), \n",
    "            StructField(\"Date_Time\", TimestampType(), True),    \n",
    "            StructField(\"day_of_week\", IntegerType(), True),    \n",
    "            StructField(\"month\", IntegerType(), True),          \n",
    "            StructField(\"year\", IntegerType(), True),           \n",
    "            StructField(\"is_month_end\", BooleanType(), True)    \n",
    "        ])\n",
    "        \n",
    "        # Create Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(data_rows, schema)\n",
    "        \n",
    "        # Write to Delta table\n",
    "        spark_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"default.bronze_financial_stocks\")\n",
    "        \n",
    "        print(\"\\nSuccessfully wrote data to bronze table\")\n",
    "        \n",
    "        # Clear data\n",
    "        del combined_data, data_rows, spark_df\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "else:\n",
    "    print(\"No data was collected\")\n",
    "\n",
    "# Final verification and get row counts\n",
    "row_counts_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Ticker,\n",
    "        Company_Name,\n",
    "        Count(*) as record_count,\n",
    "        COUNT(SMA_20) as rows_with_sma,\n",
    "        COUNT(RSI) as rows_with_rsi,\n",
    "        MIN(Date_Time) as earliest_date,\n",
    "        MAX(Date_Time) as latest_date\n",
    "    FROM default.bronze_financial_stocks \n",
    "    GROUP BY Ticker, Company_Name\n",
    "    ORDER BY Ticker\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "print(\"\\nVerifying final data - showing stocks and their record counts:\")\n",
    "row_counts_df.show(truncate=False)\n",
    "\n",
    "# Create the completion message with detailed counts\n",
    "completion_message = \"Successfully completed processing stock data.\\n\\nData summary by ticker:\\n\"\n",
    "for row in row_counts_df.collect():\n",
    "    completion_message += f\"\\n{row.Ticker} ({row.Company_Name}):\\n\"\n",
    "    completion_message += f\"- Total records: {row.record_count}\\n\"\n",
    "    completion_message += f\"- Records with technical indicators: {row.rows_with_sma}\\n\"\n",
    "    completion_message += f\"- Date range: {row.earliest_date} to {row.latest_date}\\n\"\n",
    "\n",
    "# Send completion notification\n",
    "send_message(\n",
    "    subject=\"Stock Data Processing Complete\",\n",
    "    body=completion_message\n",
    ")\n",
    "\n",
    "print(\"\\nScript completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720498e4-c902-4939-89ea-07d79915522b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStarting gold layer processing...\nDropping existing gold table if it exists...\nCreating temporary view of bronze data...\nExecuting gold layer analysis...\nGold layer table created successfully\nGenerating summary statistics...\n\nGold layer processing summary:\n+------+---------------------+------------+-------------+-----------+----------------------+--------------------+----------------+--------------+-----------------+\n|Ticker|Company_Name         |record_count|earliest_date|latest_date|avg_stock_deviation   |avg_market_deviation|overbought_count|oversold_count|volatility_events|\n+------+---------------------+------------+-------------+-----------+----------------------+--------------------+----------------+--------------+-----------------+\n|AAPL  |Apple Inc.           |2500        |2020-01-02   |2024-12-18 |-4.200728653813712E-15|-12.028627491987008 |300             |34            |288              |\n|MSFT  |Microsoft Corporation|2500        |2020-01-02   |2024-12-18 |-3.291233952040784E-15|67.1332280929173    |226             |24            |294              |\n|NVDA  |NVIDIA Corporation   |2500        |2020-01-02   |2024-12-18 |-5.766196409240365E-14|-78.0687359390662   |366             |20            |318              |\n|TSLA  |Tesla, Inc.          |2500        |2020-01-02   |2024-12-18 |-5.684341886080802E-17|22.964135338135932  |318             |114           |370              |\n+------+---------------------+------------+-------------+-----------+----------------------+--------------------+----------------+--------------+-----------------+\n\n\nGold layer processing completed!\n"
     ]
    }
   ],
   "source": [
    "# Gold Layer Processing - Main Objectives:\n",
    "# 1. Analyze stock performance and market trends\n",
    "# 2. Generate trading signals based on technical indicators\n",
    "# 3. Identify market extremes and stock-specific patterns\n",
    "# 4. Provide comprehensive market analysis for decision making\n",
    "print(\"\\nStarting gold layer processing...\")\n",
    "\n",
    "def gold_layer_processing():\n",
    "    \"\"\"\n",
    "    Analyzes relationships and patterns in financial stock data by:\n",
    "    1. Identifying monthly high/low points for market trend analysis\n",
    "    2. Calculating technical indicators for trading signals\n",
    "    3. Generating buy/sell signals based on RSI and Bollinger Bands\n",
    "    4. Computing stock-specific and market-wide statistics for comparison\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Drop existing gold table before processing\n",
    "        print(\"Dropping existing gold table if it exists...\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS default.gold_financial_stocks\")\n",
    "        \n",
    "        # Create a temporary view of the bronze table\n",
    "        print(\"Creating temporary view of bronze data...\")\n",
    "        spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW temp_bronze_stocks AS SELECT * FROM default.bronze_financial_stocks\")\n",
    "        \n",
    "        # Main analysis query\n",
    "        query = \"\"\"\n",
    "        -- RankedStocks CTE\n",
    "        -- Objective: Identify monthly price extremes for each stock\n",
    "        -- Purpose: Used to understand market cycles and monthly trading ranges\n",
    "        WITH RankedStocks AS (\n",
    "            SELECT \n",
    "                CAST(Date_Time AS DATE) as Date,\n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                Close_Price,\n",
    "                Volume_Num,\n",
    "                SMA_20,\n",
    "                RSI,\n",
    "                -- Rank prices to find monthly highs\n",
    "                ROW_NUMBER() OVER(\n",
    "                    PARTITION BY DATE_TRUNC('month', CAST(Date_Time AS DATE)) \n",
    "                    ORDER BY Close_Price DESC\n",
    "                ) as RowNumHigh,\n",
    "                -- Rank prices to find monthly lows\n",
    "                ROW_NUMBER() OVER(\n",
    "                    PARTITION BY DATE_TRUNC('month', CAST(Date_Time AS DATE)) \n",
    "                    ORDER BY Close_Price ASC\n",
    "                ) as RowNumLow\n",
    "            FROM temp_bronze_stocks\n",
    "            WHERE Date_Time >= '2020-01-01'\n",
    "        ),\n",
    "        \n",
    "        -- ExtremePoints CTE\n",
    "        -- Objective: Extract only the highest and lowest prices of each month\n",
    "        -- Purpose: Used for trend analysis and support/resistance levels\n",
    "        ExtremePoints AS (\n",
    "            SELECT *\n",
    "            FROM RankedStocks\n",
    "            WHERE RowNumHigh = 1 OR RowNumLow = 1\n",
    "        ),\n",
    "        \n",
    "        -- TechStocks CTE\n",
    "        -- Objective: Gather all technical indicators for analysis\n",
    "        -- Purpose: Combine multiple indicators for stronger trading signals\n",
    "        TechStocks AS (\n",
    "            SELECT \n",
    "                CAST(Date_Time AS DATE) as Date,\n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                Close_Price,\n",
    "                Volume_Num,\n",
    "                SMA_20,        -- Trend direction\n",
    "                RSI,           -- Overbought/Oversold conditions\n",
    "                MACD,          -- Momentum and trend changes\n",
    "                BB_Upper,      -- Volatility upper bound\n",
    "                BB_Lower       -- Volatility lower bound\n",
    "            FROM temp_bronze_stocks\n",
    "            WHERE Date_Time >= '2020-01-01'\n",
    "        ),\n",
    "        \n",
    "        -- JoinedData CTE\n",
    "        -- Objective: Combine technical data with market extremes\n",
    "        -- Purpose: Compare individual stock performance against market conditions\n",
    "        JoinedData AS (\n",
    "            SELECT \n",
    "                ts.Date,\n",
    "                ts.Ticker,\n",
    "                ts.Company_Name,\n",
    "                ts.Close_Price as StockClose,\n",
    "                ts.Volume_Num as StockVolume,\n",
    "                ts.SMA_20,\n",
    "                ts.RSI,\n",
    "                ts.MACD,\n",
    "                ts.BB_Upper,\n",
    "                ts.BB_Lower,\n",
    "                ep.Close_Price as BenchmarkClose,\n",
    "                CASE \n",
    "                    WHEN ep.RowNumHigh = 1 THEN 'High'\n",
    "                    WHEN ep.RowNumLow = 1 THEN 'Low'\n",
    "                END as MarketExtreme\n",
    "            FROM TechStocks ts\n",
    "            JOIN ExtremePoints ep \n",
    "            ON DATE_TRUNC('month', ts.Date) = DATE_TRUNC('month', ep.Date)\n",
    "        ),\n",
    "        \n",
    "        -- AggregatedData CTE\n",
    "        -- Objective: Calculate comparative statistics and averages\n",
    "        -- Purpose: Provide context for current price levels and volume\n",
    "        AggregatedData AS (\n",
    "            SELECT \n",
    "                Date,\n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                StockClose,\n",
    "                StockVolume,\n",
    "                SMA_20,\n",
    "                RSI,\n",
    "                MACD,\n",
    "                BB_Upper,\n",
    "                BB_Lower,\n",
    "                BenchmarkClose,\n",
    "                MarketExtreme,\n",
    "                -- Calculate averages for comparison\n",
    "                AVG(StockClose) OVER (PARTITION BY Ticker) as AvgStockClose,     -- Stock's historical average\n",
    "                AVG(StockVolume) OVER (PARTITION BY Ticker) as AvgStockVolume,   -- Normal trading volume\n",
    "                AVG(StockClose) OVER () as AvgMarketClose                        -- Market average\n",
    "            FROM JoinedData\n",
    "        )\n",
    "        \n",
    "        -- Final SELECT: Generate actionable insights and signals\n",
    "        -- Objective: Produce final analysis with trading signals and market context\n",
    "        SELECT \n",
    "            Date,\n",
    "            Ticker,\n",
    "            Company_Name,\n",
    "            StockClose,\n",
    "            StockVolume,\n",
    "            SMA_20,\n",
    "            RSI,\n",
    "            MACD,\n",
    "            BB_Upper,\n",
    "            BB_Lower,\n",
    "            BenchmarkClose,\n",
    "            MarketExtreme,\n",
    "            AvgStockClose,\n",
    "            AvgStockVolume,\n",
    "            AvgMarketClose,\n",
    "            -- Deviation calculations for trend strength\n",
    "            (StockClose - AvgStockClose) / NULLIF(AvgStockClose, 0) * 100 as StockDeviation,\n",
    "            (StockClose - AvgMarketClose) / NULLIF(AvgMarketClose, 0) * 100 as MarketDeviation,\n",
    "            -- RSI-based trading signals\n",
    "            CASE \n",
    "                WHEN RSI > 70 THEN 'Overbought'    -- Potential sell signal\n",
    "                WHEN RSI < 30 THEN 'Oversold'      -- Potential buy signal\n",
    "                ELSE 'Neutral'\n",
    "            END as RSI_Signal,\n",
    "            -- Bollinger Bands trading signals\n",
    "            CASE\n",
    "                WHEN StockClose > BB_Upper THEN 'Above Upper Band'   -- Price above normal range\n",
    "                WHEN StockClose < BB_Lower THEN 'Below Lower Band'   -- Price below normal range\n",
    "                ELSE 'Within Bands'                                  -- Price within normal range\n",
    "            END as BB_Signal\n",
    "        FROM AggregatedData\n",
    "        ORDER BY Date, Ticker\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Executing gold layer analysis...\")\n",
    "        # Execute the analysis query and save results to gold table\n",
    "        spark.sql(query).write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"default.gold_financial_stocks\")\n",
    "        \n",
    "        print(\"Gold layer table created successfully\")\n",
    "        \n",
    "        # Clean up temporary view\n",
    "        spark.sql(\"DROP VIEW IF EXISTS temp_bronze_stocks\")\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        print(\"Generating summary statistics...\")\n",
    "        return spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                COUNT(*) as record_count,\n",
    "                MIN(Date) as earliest_date,\n",
    "                MAX(Date) as latest_date,\n",
    "                AVG(StockDeviation) as avg_stock_deviation,\n",
    "                AVG(MarketDeviation) as avg_market_deviation,\n",
    "                COUNT(CASE WHEN RSI_Signal = 'Overbought' THEN 1 END) as overbought_count,\n",
    "                COUNT(CASE WHEN RSI_Signal = 'Oversold' THEN 1 END) as oversold_count,\n",
    "                COUNT(CASE WHEN BB_Signal != 'Within Bands' THEN 1 END) as volatility_events\n",
    "            FROM default.gold_financial_stocks\n",
    "            GROUP BY Ticker, Company_Name\n",
    "            ORDER BY Ticker\n",
    "        \"\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in gold layer processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Execute gold layer processing and get summary\n",
    "    gold_summary = gold_layer_processing()\n",
    "    print(\"\\nGold layer processing summary:\")\n",
    "    gold_summary.show(truncate=False)\n",
    "\n",
    "    # Update completion message with gold layer results\n",
    "    completion_message = \"Gold layer analysis completed with the following insights:\"\n",
    "    for row in gold_summary.collect():\n",
    "        completion_message += f\"\\n\\n{row.Ticker} ({row.Company_Name}):\\n\"\n",
    "        completion_message += f\"- Total analyzed periods: {row.record_count}\\n\"\n",
    "        completion_message += f\"- Date range: {row.earliest_date} to {row.latest_date}\\n\"\n",
    "        completion_message += f\"- Average deviation from stock mean: {row.avg_stock_deviation:.2f}%\\n\"\n",
    "        completion_message += f\"- Average deviation from market mean: {row.avg_market_deviation:.2f}%\\n\"\n",
    "        completion_message += f\"- Overbought periods: {row.overbought_count}\\n\"\n",
    "        completion_message += f\"- Oversold periods: {row.oversold_count}\\n\"\n",
    "        completion_message += f\"- Volatility events: {row.volatility_events}\"\n",
    "\n",
    "    # Send the completion notification\n",
    "    send_message(\n",
    "        subject=\"Gold Layer Processing Complete\",\n",
    "        body=completion_message\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    error_message = f\"Error in gold layer processing and reporting: {str(e)}\"\n",
    "    print(error_message)\n",
    "    send_message(\n",
    "        subject=\"Gold Layer Processing Error\",\n",
    "        body=error_message\n",
    "    )\n",
    "\n",
    "print(\"\\nGold layer processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8b30d0-ee76-470b-8e33-10fc67b9b6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nStarting gold layer processing...\nDropping existing gold table if it exists...\nCreating temporary view of bronze data...\nExecuting gold layer analysis...\nGold layer table created successfully\nGenerating summary statistics...\n\nGold layer processing summary:\n+------+---------------------+------------+-------------+-----------+----------------------+--------------------+----------------+--------------+-----------------+\n|Ticker|Company_Name         |record_count|earliest_date|latest_date|avg_stock_deviation   |avg_market_deviation|overbought_count|oversold_count|volatility_events|\n+------+---------------------+------------+-------------+-----------+----------------------+--------------------+----------------+--------------+-----------------+\n|AAPL  |Apple Inc.           |2500        |2020-01-02   |2024-12-18 |-4.200728653813712E-15|-12.028627491987008 |300             |34            |288              |\n|MSFT  |Microsoft Corporation|2500        |2020-01-02   |2024-12-18 |-3.291233952040784E-15|67.1332280929173    |226             |24            |294              |\n|NVDA  |NVIDIA Corporation   |2500        |2020-01-02   |2024-12-18 |-5.766196409240365E-14|-78.0687359390662   |366             |20            |318              |\n|TSLA  |Tesla, Inc.          |2500        |2020-01-02   |2024-12-18 |-5.684341886080802E-17|22.964135338135932  |318             |114           |370              |\n+------+---------------------+------------+-------------+-----------+----------------------+--------------------+----------------+--------------+-----------------+\n\n\nGold layer processing completed!\n"
     ]
    }
   ],
   "source": [
    "def gold_layer_investment_growth():\n",
    "    \"\"\"\n",
    "    Calculates the investment growth for each stock by analyzing daily returns.\n",
    "    This analysis helps understand:\n",
    "    1. Long-term investment performance\n",
    "    2. Compound returns over time\n",
    "    3. Comparative stock performance\n",
    "    4. Investment value evolution\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Drop existing gold table if it exists\n",
    "        print(\"Dropping existing investment growth table if it exists...\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS default.gold_investment_growth\")\n",
    "        \n",
    "        # Create a temporary view\n",
    "        print(\"Creating temporary view of bronze data...\")\n",
    "        spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW temp_bronze_stocks AS SELECT * FROM default.bronze_financial_stocks\")\n",
    "        \n",
    "        # Investment growth analysis query\n",
    "        query = \"\"\"\n",
    "        -- Base data CTE\n",
    "        -- Objective: Filter and prepare the initial dataset with proper date formatting\n",
    "        WITH BaseData AS (\n",
    "            SELECT \n",
    "                CAST(Date_Time AS DATE) as Date,\n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                Close_Price as Close\n",
    "            FROM temp_bronze_stocks\n",
    "            WHERE Date_Time >= '2020-01-01'\n",
    "            ORDER BY Ticker, Date\n",
    "        ),\n",
    "        \n",
    "        -- DailyReturns CTE\n",
    "        -- Objective: Calculate day-over-day returns for each stock\n",
    "        -- Purpose: Measure daily price changes as percentage returns\n",
    "        DailyReturns AS (\n",
    "            SELECT\n",
    "                Date,\n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                Close,\n",
    "                -- Calculate daily return using current and previous day's price\n",
    "                (Close / LAG(Close, 1) OVER (PARTITION BY Ticker ORDER BY Date)) - 1 as Daily_Return\n",
    "            FROM BaseData\n",
    "        ),\n",
    "        \n",
    "        -- CumulativeReturns CTE\n",
    "        -- Objective: Calculate cumulative investment growth\n",
    "        -- Purpose: Track the value of $1 invested at the start\n",
    "        CumulativeReturns AS (\n",
    "            SELECT\n",
    "                Date,\n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                Close,\n",
    "                Daily_Return,\n",
    "                -- Calculate cumulative value using product of (1 + daily returns)\n",
    "                EXP(SUM(LN(1 + COALESCE(Daily_Return, 0))) OVER (PARTITION BY Ticker ORDER BY Date)) \n",
    "                as Investment_Value\n",
    "            FROM DailyReturns\n",
    "        ),\n",
    "        \n",
    "        -- FinalValues CTE\n",
    "        -- Objective: Get the latest investment value for each stock\n",
    "        -- Purpose: Determine final investment performance\n",
    "        FinalValues AS (\n",
    "            SELECT\n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                FIRST_VALUE(Date) OVER (PARTITION BY Ticker ORDER BY Date) as Start_Date,\n",
    "                LAST_VALUE(Date) OVER (PARTITION BY Ticker ORDER BY Date) as End_Date,\n",
    "                LAST_VALUE(Investment_Value) OVER (PARTITION BY Ticker ORDER BY Date) as Final_Value,\n",
    "                MIN(Investment_Value) OVER (PARTITION BY Ticker) as Lowest_Value,\n",
    "                MAX(Investment_Value) OVER (PARTITION BY Ticker) as Highest_Value,\n",
    "                AVG(Daily_Return) OVER (PARTITION BY Ticker) * 252 as Annualized_Return, -- 252 trading days\n",
    "                STDDEV(Daily_Return) OVER (PARTITION BY Ticker) * SQRT(252) as Annualized_Volatility\n",
    "            FROM CumulativeReturns\n",
    "        )\n",
    "        \n",
    "        -- Final SELECT: Generate comprehensive investment analysis\n",
    "        -- Objective: Provide complete investment performance metrics\n",
    "        SELECT DISTINCT\n",
    "            Ticker,\n",
    "            Company_Name,\n",
    "            Start_Date,\n",
    "            End_Date,\n",
    "            ROUND(Final_Value, 2) as Final_Investment_Value,\n",
    "            ROUND(Lowest_Value, 2) as Minimum_Investment_Value,\n",
    "            ROUND(Highest_Value, 2) as Maximum_Investment_Value,\n",
    "            ROUND(Annualized_Return * 100, 2) as Annualized_Return_Pct,\n",
    "            ROUND(Annualized_Volatility * 100, 2) as Annualized_Volatility_Pct,\n",
    "            ROUND((Final_Value - 1) * 100, 2) as Total_Return_Pct,\n",
    "            ROUND((Highest_Value / Lowest_Value - 1) * 100, 2) as Max_Drawdown_Pct\n",
    "        FROM FinalValues\n",
    "        ORDER BY Final_Investment_Value DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Executing investment growth analysis...\")\n",
    "        # Execute analysis and save to gold table\n",
    "        spark.sql(query).write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"default.gold_investment_growth\")\n",
    "            \n",
    "        print(\"Investment growth table created successfully\")\n",
    "        \n",
    "        # Clean up temporary view\n",
    "        spark.sql(\"DROP VIEW IF EXISTS temp_bronze_stocks\")\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        print(\"Generating investment summary...\")\n",
    "        return spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                Ticker,\n",
    "                Company_Name,\n",
    "                Final_Investment_Value,\n",
    "                Total_Return_Pct,\n",
    "                Annualized_Return_Pct,\n",
    "                Annualized_Volatility_Pct,\n",
    "                Max_Drawdown_Pct,\n",
    "                Start_Date,\n",
    "                End_Date\n",
    "            FROM default.gold_investment_growth\n",
    "            ORDER BY Final_Investment_Value DESC\n",
    "        \"\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in investment growth analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Execute gold layer processing and get summary\n",
    "    print(\"\\nStarting gold layer processing...\")\n",
    "    gold_summary = gold_layer_processing()\n",
    "    print(\"\\nGold layer processing summary:\")\n",
    "    gold_summary.show(truncate=False)\n",
    "\n",
    "    # Create comprehensive message with explanations\n",
    "    completion_message = \"Gold Layer Analysis Completed\\n\\n\"\n",
    "    completion_message += \"METRIC EXPLANATIONS:\\n\"\n",
    "    completion_message += \"1. Average Deviation from Stock Mean:\\n\"\n",
    "    completion_message += \"   • Shows how far the stock's price deviates from its own historical average\\n\"\n",
    "    completion_message += \"   • Values close to 0% suggest stable trading around historical average\\n\"\n",
    "    completion_message += \"   • Larger deviations indicate significant price movements\\n\\n\"\n",
    "    completion_message += \"2. Average Deviation from Market Mean:\\n\"\n",
    "    completion_message += \"   • Compares stock's price to the average of all analyzed stocks\\n\"\n",
    "    completion_message += \"   • Negative values indicate trading below market average\\n\"\n",
    "    completion_message += \"   • Positive values indicate trading above market average\\n\\n\"\n",
    "    completion_message += \"3. Overbought/Oversold Periods:\\n\"\n",
    "    completion_message += \"   • Based on RSI (Relative Strength Index)\\n\"\n",
    "    completion_message += \"   • Overbought (RSI > 70): Indicates strong upward momentum\\n\"\n",
    "    completion_message += \"   • Oversold (RSI < 30): Indicates potential buying opportunities\\n\\n\"\n",
    "    completion_message += \"4. Volatility Events:\\n\"\n",
    "    completion_message += \"   • Times price moved outside Bollinger Bands\\n\"\n",
    "    completion_message += \"   • Higher numbers indicate more volatile price action\\n\\n\"\n",
    "    completion_message += \"DETAILED ANALYSIS BY STOCK:\\n\"\n",
    "\n",
    "    for row in gold_summary.collect():\n",
    "        completion_message += f\"\\n{row.Ticker} ({row.Company_Name}):\\n\"\n",
    "        completion_message += f\"- Total analyzed periods: {row.record_count}\\n\"\n",
    "        completion_message += f\"- Date range: {row.earliest_date} to {row.latest_date}\\n\"\n",
    "        completion_message += f\"- Average deviation from stock mean: {row.avg_stock_deviation:.2f}%\\n\"\n",
    "        completion_message += f\"- Average deviation from market mean: {row.avg_market_deviation:.2f}%\\n\"\n",
    "        completion_message += f\"- Overbought periods: {row.overbought_count}\\n\"\n",
    "        completion_message += f\"- Oversold periods: {row.oversold_count}\\n\"\n",
    "        completion_message += f\"- Volatility events: {row.volatility_events}\\n\"\n",
    "\n",
    "        # Add stock-specific interpretation\n",
    "        completion_message += \"\\nKey Insights:\\n\"\n",
    "        \n",
    "        # Price stability interpretation\n",
    "        if abs(row.avg_stock_deviation) < 5:\n",
    "            completion_message += \"• Stock shows stable price behavior around its average\\n\"\n",
    "        else:\n",
    "            completion_message += \"• Stock shows notable price movements from its average\\n\"\n",
    "        \n",
    "        # Market comparison interpretation\n",
    "        if row.avg_market_deviation > 0:\n",
    "            completion_message += f\"• Trades {abs(row.avg_market_deviation):.1f}% above market average\\n\"\n",
    "        else:\n",
    "            completion_message += f\"• Trades {abs(row.avg_market_deviation):.1f}% below market average\\n\"\n",
    "        \n",
    "        # Momentum interpretation\n",
    "        if row.overbought_count > row.oversold_count * 2:\n",
    "            completion_message += \"• Strong upward momentum dominates\\n\"\n",
    "        elif row.oversold_count > row.overbought_count * 2:\n",
    "            completion_message += \"• Shows significant downward pressure\\n\"\n",
    "        \n",
    "        # Volatility interpretation\n",
    "        if row.volatility_events > 300:\n",
    "            completion_message += \"• Exhibits high volatility, requires careful monitoring\\n\"\n",
    "        else:\n",
    "            completion_message += \"• Shows manageable volatility levels\\n\"\n",
    "\n",
    "    # Send completion notification\n",
    "    send_message(\n",
    "        subject=\"Gold Layer Analysis Complete\",\n",
    "        body=completion_message\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    error_message = f\"Error in gold layer processing and reporting: {str(e)}\"\n",
    "    print(error_message)\n",
    "    send_message(\n",
    "        subject=\"Gold Layer Analysis Error\",\n",
    "        body=error_message\n",
    "    )\n",
    "\n",
    "print(\"\\nGold layer processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560dfe57-2db6-4c38-a6d0-d7d372245a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-86d4a31e-aa0c-43b8-9d40-cd/.ipykernel/27517/command-4307446457723566-3592408352:93: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and displaying visualizations...\n\nGenerating dashboard for AAPL...\n\nGenerating dashboard for MSFT...\n\nGenerating dashboard for NVDA...\n\nGenerating dashboard for TSLA...\n\nSending email with visualizations...\nVisualizations have been generated, displayed, and sent via email!\n"
     ]
    }
   ],
   "source": [
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.image import MIMEImage\n",
    "from email.mime.text import MIMEText\n",
    "import io\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use Agg backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def send_message_with_images(subject=\"Email by Python\", body=\"Default by Python\", figures=None):\n",
    "    \"\"\"\n",
    "    Enhanced version of send_message that handles matplotlib figures directly\n",
    "    figures: list of tuples (figure, name)\n",
    "    \"\"\"\n",
    "    # Read credentials\n",
    "    with open('/Volumes/workspace/default/data/gmail.txt', 'r') as file:\n",
    "        password = file.read().strip()\n",
    "    \n",
    "    with open('/Volumes/workspace/default/data/sender.txt', 'r') as file:\n",
    "        sender_email = file.read().strip()\n",
    "        \n",
    "    with open('/Volumes/workspace/default/data/receiver.txt', 'r') as file:\n",
    "        receiver_email = file.read().strip()\n",
    "    \n",
    "    # Create message container\n",
    "    message = MIMEMultipart('related')\n",
    "    message['Subject'] = subject\n",
    "    message['From'] = f\"Databricks - Extractor<{sender_email}>\"\n",
    "    message['To'] = receiver_email\n",
    "    \n",
    "    # Create the HTML body\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "      <body>\n",
    "        <h2>Technical Analysis Report</h2>\n",
    "        \n",
    "        <h3>Market Overview:</h3>\n",
    "        <p>- The market deviation chart shows the relative performance of each stock compared to the market average.</p>\n",
    "        <p>- Positive deviations indicate outperformance, negative deviations indicate underperformance.</p>\n",
    "        \n",
    "        <h3>Individual Stock Analysis:</h3>\n",
    "        <p>Each technical dashboard shows:</p>\n",
    "        <ol>\n",
    "            <li>Price movement with Bollinger Bands</li>\n",
    "            <li>RSI indicator (overbought >70, oversold <30)</li>\n",
    "            <li>MACD trend indicator</li>\n",
    "        </ol>\n",
    "        \n",
    "        <h3>Charts:</h3>\n",
    "        {body}\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Record the MIME types of text/html and image\n",
    "    html_part = MIMEText(html, 'html')\n",
    "    message.attach(html_part)\n",
    "    \n",
    "    # Attach figures\n",
    "    if figures:\n",
    "        for i, (fig, name) in enumerate(figures):\n",
    "            # Save figure to bytes buffer\n",
    "            buf = io.BytesIO()\n",
    "            fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')\n",
    "            buf.seek(0)\n",
    "            img_data = buf.getvalue()\n",
    "            \n",
    "            image = MIMEImage(img_data)\n",
    "            image.add_header('Content-ID', f'<image{i}>')\n",
    "            image.add_header('Content-Disposition', 'inline', filename=name)\n",
    "            message.attach(image)\n",
    "            buf.close()\n",
    "    \n",
    "    # Send email\n",
    "    server = smtplib.SMTP('smtp.gmail.com:587')\n",
    "    server.ehlo_or_helo_if_needed()\n",
    "    server.starttls()\n",
    "    server.ehlo_or_helo_if_needed()\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(f\"Databricks - Extractor<{sender_email}>\", receiver_email, message.as_string())\n",
    "    server.quit()\n",
    "\n",
    "def create_and_send_visualizations():\n",
    "    \"\"\"Create visualizations, display them, and send via email\"\"\"\n",
    "    # Get the data from gold table\n",
    "    gold_df = spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM default.gold_financial_stocks\n",
    "        ORDER BY Date, Ticker\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    figures = []\n",
    "    html_images = []\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    print(\"Generating and displaying visualizations...\")\n",
    "    \n",
    "    # 1. Market Deviation Analysis\n",
    "    fig1 = plt.figure(figsize=(12, 6))\n",
    "    for ticker in gold_df['Ticker'].unique():\n",
    "        stock_data = gold_df[gold_df['Ticker'] == ticker]\n",
    "        plt.plot(stock_data['Date'], stock_data['MarketDeviation'], label=ticker)\n",
    "    \n",
    "    plt.title('Stock Deviations from Market Average')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Deviation (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "    figures.append((fig1, 'market_deviation.png'))\n",
    "    html_images.append('<img src=\"cid:image0\" width=\"800px\"><br><br>')\n",
    "    \n",
    "    # 2. Technical Analysis Dashboard for each stock\n",
    "    for i, ticker in enumerate(gold_df['Ticker'].unique(), 1):\n",
    "        print(f\"\\nGenerating dashboard for {ticker}...\")\n",
    "        stock_data = gold_df[gold_df['Ticker'] == ticker].copy()\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12), height_ratios=[2, 1, 1])\n",
    "        fig.suptitle(f'Technical Analysis Dashboard - {ticker}', fontsize=16)\n",
    "        \n",
    "        # Price and Bollinger Bands\n",
    "        ax1.plot(stock_data['Date'], stock_data['StockClose'], label='Price', color='blue')\n",
    "        ax1.plot(stock_data['Date'], stock_data['BB_Upper'], '--', label='Upper BB', color='gray', alpha=0.7)\n",
    "        ax1.plot(stock_data['Date'], stock_data['BB_Lower'], '--', label='Lower BB', color='gray', alpha=0.7)\n",
    "        ax1.fill_between(stock_data['Date'], stock_data['BB_Upper'], stock_data['BB_Lower'], alpha=0.1)\n",
    "        ax1.set_title('Price and Bollinger Bands')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # RSI\n",
    "        ax2.plot(stock_data['Date'], stock_data['RSI'], color='purple')\n",
    "        ax2.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
    "        ax2.axhline(y=30, color='g', linestyle='--', alpha=0.5)\n",
    "        ax2.set_title('RSI')\n",
    "        ax2.set_ylim(0, 100)\n",
    "        \n",
    "        # MACD\n",
    "        ax3.plot(stock_data['Date'], stock_data['MACD'], label='MACD', color='blue')\n",
    "        ax3.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "        ax3.set_title('MACD')\n",
    "        ax3.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "        \n",
    "        figures.append((fig, f'technical_dashboard_{ticker}.png'))\n",
    "        html_images.append(f'<img src=\"cid:image{i}\" width=\"800px\"><br><br>')\n",
    "\n",
    "    # Send email with images\n",
    "    print(\"\\nSending email with visualizations...\")\n",
    "    send_message_with_images(\n",
    "        subject=\"Stock Market Technical Analysis Charts\",\n",
    "        body=''.join(html_images),\n",
    "        figures=figures\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    for fig, _ in figures:\n",
    "        plt.close(fig)\n",
    "    \n",
    "    print(\"Visualizations have been generated, displayed, and sent via email!\")\n",
    "\n",
    "# Execute the function\n",
    "create_and_send_visualizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b82bc4-7ca1-40cd-9ffa-4af77ec6ffb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import smtplib, time, datetime, inspect, os\n",
    "from time import sleep\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_message(subject=\"Email by Python\", body=\"Default by Python\"):\n",
    "    \"\"\"By Ricardo Kazuo\"\"\"\n",
    "    # Read credentials and addresses from files\n",
    "    with open('/Volumes/workspace/default/data/gmail.txt', 'r') as file:\n",
    "        password = file.read().strip()\n",
    "    \n",
    "    with open('/Volumes/workspace/default/data/sender.txt', 'r') as file:\n",
    "        sender_email = file.read().strip()\n",
    "        \n",
    "    with open('/Volumes/workspace/default/data/receiver.txt', 'r') as file:\n",
    "        receiver_email = file.read().strip()\n",
    "        \n",
    "    message = MIMEText(body)\n",
    "    message['to'] = receiver_email\n",
    "    message['from'] = f\"Databricks - Extractor<{sender_email}>\"\n",
    "    message['subject'] = subject\n",
    "    server = smtplib.SMTP('smtp.gmail.com:587')\n",
    "    server.ehlo_or_helo_if_needed()\n",
    "    server.starttls()\n",
    "    server.ehlo_or_helo_if_needed()\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(f\"Databricks - Extractor<{sender_email}>\", receiver_email, message.as_string())\n",
    "    server.quit()\n",
    "\n",
    "# Start timing and send start notification\n",
    "start_time = time.time()\n",
    "\n",
    "# Main data processing code\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    a.*\n",
    "FROM workspace.default.gold_financial_stocks a\n",
    "\"\"\")\n",
    "\n",
    "# Get data profile before writing\n",
    "num_rows = df.count()\n",
    "num_columns = len(df.columns)\n",
    "estimated_size_bytes = num_rows * num_columns * 10\n",
    "estimated_size_mb = round(estimated_size_bytes / (1024 * 1024), 2)\n",
    "estimated_compressed_size_mb = round(estimated_size_mb * 0.15, 2)\n",
    "\n",
    "# Generate timestamp for the file name\n",
    "current_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "base_path = f\"/Volumes/workspace/default/data/{current_date}.csv.gz\"\n",
    "\n",
    "# Write with compression\n",
    "df.coalesce(1).write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(base_path)\n",
    "\n",
    "# Get the actual file path including the part file name\n",
    "output_files = dbutils.fs.ls(base_path)\n",
    "part_file_path = [f for f in output_files if f.name.startswith('part-')][0].path\n",
    "\n",
    "# Clean up the path (remove 'dbfs:' prefix if present)\n",
    "if part_file_path.startswith('dbfs:'):\n",
    "    part_file_path = part_file_path[5:]\n",
    "\n",
    "# End timing and send completion notification with data profile\n",
    "end_time = time.time()\n",
    "formatted_end_time = datetime.datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "execution_time = round(end_time - start_time, 2)\n",
    "\n",
    "# Get the workspace ID and construct the download URL\n",
    "workspace_id = \"913057a4-85b7\"\n",
    "download_url = f\"https://dbc-{workspace_id}.cloud.databricks.com/ajax-api/2.0/fs/files{part_file_path}\"\n",
    "\n",
    "report = f\"\"\"\n",
    "Data Export Summary:\n",
    "-------------------\n",
    "Number of Rows: {num_rows:,}\n",
    "Number of Columns: {num_columns}\n",
    "Compression Ratio: {round(estimated_size_mb/estimated_compressed_size_mb, 2)}:1\n",
    "Execution Time: {execution_time} seconds\n",
    "Location: {part_file_path}\n",
    "\n",
    "Download Link:\n",
    "{download_url}\n",
    "\"\"\"\n",
    "\n",
    "send_message(\n",
    "    subject=f\"{inspect.currentframe().f_code.co_name} END: {formatted_end_time} (Execution time: {execution_time} seconds)\",\n",
    "    body=report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c140a7cd-8255-4364-aee4-8e200b3bdeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import smtplib, time, datetime, inspect, os\n",
    "from time import sleep\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_message(subject=\"Email by Python\", body=\"Default by Python\"):\n",
    "    \"\"\"By Ricardo Kazuo\"\"\"\n",
    "    # Read credentials and addresses from files\n",
    "    with open('/Volumes/workspace/default/data/gmail.txt', 'r') as file:\n",
    "        password = file.read().strip()\n",
    "    \n",
    "    with open('/Volumes/workspace/default/data/sender.txt', 'r') as file:\n",
    "        sender_email = file.read().strip()\n",
    "        \n",
    "    with open('/Volumes/workspace/default/data/receiver.txt', 'r') as file:\n",
    "        receiver_email = file.read().strip()\n",
    "        \n",
    "    message = MIMEText(body)\n",
    "    message['to'] = receiver_email\n",
    "    message['from'] = f\"Databricks - Extractor<{sender_email}>\"\n",
    "    message['subject'] = subject\n",
    "    server = smtplib.SMTP('smtp.gmail.com:587')\n",
    "    server.ehlo_or_helo_if_needed()\n",
    "    server.starttls()\n",
    "    server.ehlo_or_helo_if_needed()\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(f\"Databricks - Extractor<{sender_email}>\", receiver_email, message.as_string())\n",
    "    server.quit()\n",
    "\n",
    "# Start timing and send start notification\n",
    "start_time = time.time()\n",
    "\n",
    "# Main data processing code\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    a.*\n",
    "FROM workspace.default.gold_financial_stocks a \n",
    "\"\"\")\n",
    "\n",
    "# Get data profile before writing\n",
    "num_rows = df.count()\n",
    "num_columns = len(df.columns)\n",
    "estimated_size_bytes = num_rows * num_columns * 10\n",
    "estimated_size_mb = round(estimated_size_bytes / (1024 * 1024), 2)\n",
    "estimated_compressed_size_mb = round(estimated_size_mb * 0.15, 2)\n",
    "\n",
    "# Generate timestamp for the file name\n",
    "current_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "base_path = f\"/Volumes/workspace/default/data2/{current_date}.csv.gz\"\n",
    "\n",
    "# Write with compression\n",
    "df.coalesce(1).write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(base_path)\n",
    "\n",
    "# Get the actual file path including the part file name\n",
    "output_files = dbutils.fs.ls(base_path)\n",
    "part_file_path = [f for f in output_files if f.name.startswith('part-')][0].path\n",
    "\n",
    "# Clean up the path (remove 'dbfs:' prefix if present)\n",
    "if part_file_path.startswith('dbfs:'):\n",
    "    part_file_path = part_file_path[5:]\n",
    "\n",
    "# End timing and send completion notification with data profile\n",
    "end_time = time.time()\n",
    "formatted_end_time = datetime.datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "execution_time = round(end_time - start_time, 2)\n",
    "\n",
    "# Get the workspace ID and construct the download URL\n",
    "workspace_id = \"913057a4-85b7\"\n",
    "download_url = f\"https://dbc-{workspace_id}.cloud.databricks.com/ajax-api/2.0/fs/files{part_file_path}\"\n",
    "\n",
    "report = f\"\"\"\n",
    "Data Export Summary:\n",
    "-------------------\n",
    "Number of Rows: {num_rows:,}\n",
    "Number of Columns: {num_columns}\n",
    "Compression Ratio: {round(estimated_size_mb/estimated_compressed_size_mb, 2)}:1\n",
    "Execution Time: {execution_time} seconds\n",
    "Location: {part_file_path}\n",
    "\n",
    "Download Link:\n",
    "{download_url}\n",
    "\"\"\"\n",
    "\n",
    "send_message(\n",
    "    subject=f\"{inspect.currentframe().f_code.co_name} END: {formatted_end_time} (Execution time: {execution_time} seconds)\",\n",
    "    body=report\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "First",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}